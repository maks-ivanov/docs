---
title: "main.py"
---

## High-level description
This script processes a PDF file of Shakespeare's "Hamlet", splits it into manageable chunks, and outputs a specific chunk based on a provided index. It uses the LlamaIndex library for PDF reading and text splitting, and the tiktoken library for tokenization.

## Symbols

### Main Script
#### Description
The main script reads the "Hamlet.pdf" file, combines all the text, splits it into chunks, and prints a specific chunk based on the provided index argument.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| sys.argv[1] | int | Index of the chunk to be printed |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| stdout | str | The selected chunk of text or "No more content" message |

#### Internal Logic
1. Import required libraries and modules.
2. Get the index from command-line arguments.
3. Load the "Hamlet.pdf" file using PyMuPDFReader.
4. Combine all document text into a single string.
5. Create a TokenTextSplitter with specified parameters.
6. Split the combined text into chunks.
7. Check if the provided index is valid.
8. Print the chunk at the specified index or "No more content" if index is out of range.

## Dependencies
| Dependency | Purpose |
|:-----------|:--------|
| tiktoken | For tokenization of text |
| sys | For accessing command-line arguments |
| llama_index.readers.file.PyMuPDFReader | For reading PDF files |
| llama_index.core.node_parser.TokenTextSplitter | For splitting text into chunks |

## Configuration
| Option | Type | Default | Description |
|:-------|:-----|:--------|:------------|
| chunk_size | int | 10000 | The maximum number of tokens per chunk |
| chunk_overlap | int | 10 | The number of overlapping tokens between chunks |
| tokenizer | function | tiktoken.encoding_for_model("gpt-4").encode | The tokenization function used for splitting |

## Error Handling
The script exits with a status code of 0 and prints "No more content" if the provided index is out of range for the generated chunks.