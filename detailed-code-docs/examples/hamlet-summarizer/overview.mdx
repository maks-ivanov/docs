---
title: "Overview"
---

## High-level description
This directory contains an example project called "Hamlet Summarizer" that demonstrates how to summarize large documents in chunks using GPT-based models. The project specifically uses Shakespeare's play "Hamlet" as an example, splitting it into manageable chunks and summarizing each chunk progressively.

## What does it do?
The Hamlet Summarizer takes a large document (in this case, the play "Hamlet") and breaks it down into smaller, manageable pieces. It then summarizes each piece one by one, building up a complete summary of the entire document. This approach is particularly useful for documents that are too large to fit within the context window of language models like GPT-4.

The process works as follows:
1. The PDF of "Hamlet" is ingested and split into chunks of approximately 10,000 tokens each.
2. A summarizer tool processes each chunk sequentially, considering the summaries of previous chunks.
3. The tool builds up a comprehensive summary of the entire play, chunk by chunk.

This method can be adapted for summarizing any large document, not just "Hamlet."

## Entry points
The main entry point for this project is the `hamlet-summarizer.gpt` file (not provided in the summary, but mentioned in the README). This file orchestrates the entire summarization process.

The project is organized into two main components:
1. A Python script (`main.py`) that handles the PDF ingestion, text splitting, and chunk retrieval.
2. A summarizer tool (likely defined in the `hamlet-summarizer.gpt` file) that processes and summarizes each chunk of text.

The data flow starts with the PDF being processed by `main.py`, which splits it into chunks. These chunks are then fed into the summarizer tool, which progressively builds up the summary of the entire document.

## Key Files
1. `README.md`: Provides an overview of the project, its design, and instructions for running the example.
2. `main.py`: A Python script that handles PDF processing, text splitting, and chunk retrieval.
3. `Hamlet.pdf`: The source document (Shakespeare's play "Hamlet") used for this example.
4. `requirements.txt`: Lists the Python packages required to run the example.

## Dependencies
The project relies on several external libraries:

1. OpenAI API: Used for the GPT-based summarization (version not specified).
2. tiktoken (version not specified): Used for tokenization of text.
3. llama_index (version not specified):
   - PyMuPDFReader: Used for reading PDF files.
   - TokenTextSplitter: Used for splitting text into chunks.

These dependencies were likely chosen for their specific functionalities:
- OpenAI API provides access to powerful language models for summarization.
- tiktoken is OpenAI's official tokenizer, ensuring consistency with their models.
- llama_index offers tools for document processing and chunking, which are essential for handling large documents.

## Configuration
The project uses the following configuration:

1. Environment Variables:
   - `OPENAI_API_KEY`: Must be set to your OpenAI API key before running the example.

2. Python Virtual Environment:
   - The README recommends creating and activating a Python virtual environment before installing dependencies and running the example.

3. Chunk Configuration (in `main.py`):
   - `chunk_size`: Set to 10000 tokens, determining the maximum size of each text chunk.
   - `chunk_overlap`: Set to 10 tokens, allowing for some overlap between chunks to maintain context.

To run the example, users need to:
1. Create and activate a Python virtual environment.
2. Install dependencies from `requirements.txt`.
3. Set the `OPENAI_API_KEY` environment variable.
4. Run the command: `gptscript --disable-cache hamlet-summarizer.gpt`

This configuration allows for flexibility in processing different documents and adjusting the chunking strategy as needed.