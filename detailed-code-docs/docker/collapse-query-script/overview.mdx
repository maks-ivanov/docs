---
title: "Overview"
---

## High-level description
This directory contains a Python script designed to optimize search query analytics by collapsing similar queries in a ClickHouse database. The main component is the `collapse_queries.py` script, which identifies and removes partial queries that are prefixes of longer, more complete queries within a specified time window.

## What does it do?
The script processes search queries stored in a ClickHouse database to improve the quality of analytics data. It works by:

1. Connecting to the ClickHouse database and retrieving a list of all datasets from the `search_queries` table.
2. For each dataset, it fetches search queries in batches, starting from the last processed timestamp.
3. It then analyzes these queries to identify partial queries that are prefixes of longer queries and occur within 10 seconds of each other.
4. The identified partial queries are marked as duplicates in the database.
5. The process continues until all queries in the dataset are processed or no new queries are found.
6. Finally, it updates the last processed timestamp for each dataset in the `last_collapsed_dataset` table.

This optimization helps to reduce redundancy in the stored data and prevents partial queries from skewing analytics results.

## Key Files
1. `collapse_queries.py`: The main Python script that implements the query collapsing logic.

### collapse_queries.py

This script contains several key functions:

- `get_search_queries`: Retrieves search queries for a specific dataset from the database.
- `get_datasets`: Fetches a list of all dataset IDs from the `search_queries` table.
- `get_dataset_last_collapsed`: Retrieves the timestamp of the last collapse operation for a dataset.
- `set_dataset_last_collapsed`: Updates the last collapse timestamp for a dataset.
- `collapse_queries`: Identifies partial queries that should be marked as duplicates, considering a 10-second time window.
- `insert_duplicate_rows`: Inserts identified duplicate rows back into the database with the `is_duplicate` flag set to 1.
- `main`: Orchestrates the overall query collapsing process for all datasets.

The script uses a sophisticated algorithm to compare queries within a specified range and time window, identifying duplicates based on string similarity and timestamps. It processes queries in batches of 5000 to manage memory usage efficiently.

## Dependencies
The script relies on the following Python libraries:

- `clickhouse_driver`: For connecting to and interacting with the ClickHouse database.
- `uuid`: For handling UUID data types.
- `datetime`: For working with timestamps and time calculations.
- `typing`: For type hinting and improving code readability.

## Configuration
The script uses environment variables for configuration:

- `CLICKHOUSE_DSN`: The connection string for the ClickHouse database.

Example usage:

```python
CLICKHOUSE_DSN = "clickhouse://user:password@host:port/database"
```

## Error Handling and Logging
The script includes basic error handling with a try-except block in the main function to catch and print any exceptions that occur during execution. However, it could benefit from more comprehensive logging for better tracking of the collapsing process.

## Future Improvements
Several potential improvements could enhance the functionality and performance of this script:

1. Implement parallel processing for multiple datasets to improve performance.
2. Add more comprehensive logging for better tracking of the collapsing process.
3. Implement a more sophisticated duplicate detection algorithm, possibly using fuzzy matching.
4. Add command-line arguments for customizing behavior (e.g., time window, look range).
5. Implement a dry-run mode to preview changes without modifying the database.

These improvements would make the script more flexible, efficient, and easier to monitor and debug.