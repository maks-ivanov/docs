---
title: "collapse_queries.py"
---

## High-level description
This Python script, `collapse_queries.py`, is designed to process and collapse search queries from a ClickHouse database. It identifies and marks duplicate or similar queries within a specified time window and dataset, updating the database accordingly.

## Table of contents
- Imports and type definitions
- Database interaction functions
- Query collapsing logic
- Main execution function

## Code Structure
The script is organized into several functions that handle different aspects of the query collapsing process. The main function orchestrates the overall flow, calling other functions to fetch data, process queries, and update the database.

## Symbols

### get_search_queries
#### Description
Retrieves search queries from the ClickHouse database for a specific dataset.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| client | ClickHouseClient | ClickHouse database client |
| dataset_id | uuid.UUID | ID of the dataset to query |
| limit | int | Maximum number of queries to retrieve (default: 5000) |
| offset | Optional[datetime.datetime] | Timestamp to start retrieving queries from |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result_rows | List[Tuple] | List of query data rows |

#### Internal Logic
Constructs and executes a SQL query to fetch search queries, optionally using an offset timestamp.

### get_datasets
#### Description
Retrieves all unique dataset IDs from the search_queries table.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| client | ClickHouseClient | ClickHouse database client |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result_rows | List[Tuple] | List of dataset IDs |

### get_dataset_last_collapsed
#### Description
Retrieves the timestamp of the last collapsed query for a specific dataset.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| client | ClickHouseClient | ClickHouse database client |
| dataset_id | uuid.UUID | ID of the dataset to query |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| last_collapsed | Optional[datetime.datetime] | Timestamp of the last collapsed query |

### set_dataset_last_collapsed
#### Description
Updates the last collapsed timestamp for a specific dataset.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| client | ClickHouseClient | ClickHouse database client |
| dataset_id | uuid.UUID | ID of the dataset to update |
| last_collapsed | datetime.datetime | New last collapsed timestamp |

### collapse_queries
#### Description
Identifies duplicate or similar queries within a specified time window and range.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| rows | List[Tuple] | List of query data rows |
| look_range | int | Number of queries to look at before and after (default: 10) |
| time_window | int | Time window in seconds to consider for duplicates (default: 10) |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| rows_to_be_deleted | List[Tuple] | List of query rows to be marked as duplicates |

#### Internal Logic
Compares queries within the specified range and time window, identifying duplicates based on string similarity and timestamps.

### insert_duplicate_rows
#### Description
Inserts identified duplicate rows back into the database with the is_duplicate flag set to 1.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| client | ClickHouseClient | ClickHouse database client |
| rows | List[Tuple] | List of duplicate query rows to insert |

### main
#### Description
Orchestrates the overall query collapsing process for all datasets.

#### Internal Logic
1. Connects to the ClickHouse database
2. Retrieves all datasets
3. For each dataset:
   - Fetches the last collapsed timestamp
   - Retrieves and processes queries in batches
   - Identifies and marks duplicate queries
   - Updates the last collapsed timestamp
4. Optimizes the affected tables

## Error Handling
The main function includes a try-except block to catch and print any exceptions that occur during execution.

## Future Improvements
- Implement parallel processing for multiple datasets to improve performance
- Add logging for better tracking of the collapsing process
- Implement a more sophisticated duplicate detection algorithm, possibly using fuzzy matching
- Add command-line arguments for customizing behavior (e.g., time window, look range)
- Implement a dry-run mode to preview changes without modifying the database