---
title: "Overview"
---

## High-level description
This directory contains Terraform configurations and related files for setting up a Kubernetes cluster on AWS, along with an embedding server and associated services. The main components include AWS infrastructure setup, Kubernetes installation, and configuration of various text embedding and reranking models using Docker.

## What does it do?
This codebase sets up a complete infrastructure for running text embedding and reranking services on AWS. Here's a breakdown of the main functionalities:

1. It provisions the necessary AWS resources for running a Kubernetes cluster.
2. It installs and configures Kubernetes on the AWS infrastructure.
3. It sets up an IAM policy for the AWS Load Balancer Controller, allowing it to manage Application Load Balancers (ALBs) and Network Load Balancers (NLBs) in the Kubernetes cluster.
4. It defines a set of Docker services for running various text embedding and reranking models, including Jina AI embeddings, BAAI/bge-m3, BAAI/bge-reranker-large, and SPLADE models for both document and query embedding.
5. It configures an EC2 instance to serve as an embedding server, setting up DNS, user groups, and a custom user with specific permissions and SSH access.

## Entry points
The main entry points for this codebase are:

1. `Readme.md`: Provides instructions for setting up the Kubernetes cluster on AWS.
2. `alb/policy.json`: Defines the IAM policy for the AWS Load Balancer Controller.
3. `docker-compose.yml`: Specifies the Docker services for the text embedding and reranking models.
4. `embedding_server_cloud_init.yaml`: Configures the EC2 instance for the embedding server.

The workflow typically starts with following the instructions in the Readme.md file to set up the AWS account and Kubernetes cluster. Then, the IAM policy is applied to enable the Load Balancer Controller. The Docker services are deployed to run the embedding and reranking models, and finally, the EC2 instance is configured as the embedding server.

## Dependencies
The codebase relies on several external dependencies:

1. AWS services: EC2, IAM, Elastic Load Balancing, WAF, Shield, ACM, Cognito
2. Kubernetes
3. Docker and Docker Compose
4. Hugging Face Text Embeddings Inference container (ghcr.io/huggingface/text-embeddings-inference:turing-1.2)
5. Various AI models: Jina AI embeddings, BAAI/bge-m3, BAAI/bge-reranker-large, naver/efficient-splade-VI-BT-large-doc, naver/efficient-splade-VI-BT-large-query

## Configuration
The main configuration files in this directory are:

1. `alb/policy.json`: Configures the IAM policy for the AWS Load Balancer Controller.
2. `docker-compose.yml`: Defines the Docker services for the embedding and reranking models, including port mappings, volume mounts, and runtime settings.
3. `embedding_server_cloud_init.yaml`: Configures the EC2 instance for the embedding server, including DNS settings, user groups, and SSH access.

Key configurable fields include:

- In `docker-compose.yml`:
  - Docker image versions
  - Port mappings for each service
  - Volume mounts for data persistence
  - GPU runtime settings

- In `embedding_server_cloud_init.yaml`:
  - DNS nameservers
  - User groups and permissions
  - SSH public key for the 'dev' user

Here's an example of how the Docker services are configured:

```yaml
jina:
  image: ghcr.io/huggingface/text-embeddings-inference:turing-1.2
  command: --model-id jinaai/jina-embeddings-v2-base-en
  ports:
    - 7000:80
  volumes:
    - ./data:/data
  runtime: nvidia
```

And an example of the EC2 instance configuration:

```yaml
users:
  - name: dev
    groups: [docker, dev]
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    shell: /bin/bash
    ssh_authorized_keys:
      - ${ssh_key}
```

These configurations allow for flexibility in setting up the infrastructure and services according to specific requirements.