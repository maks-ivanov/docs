---
title: "file_operator.rs"
---

## High-level description
The `file_operator.rs` file contains functions for handling file uploads, downloads, and deletions. It interacts with an S3 bucket for file storage and a PostgreSQL database for metadata management. Additionally, it utilizes Redis for queuing file processing tasks and ClickHouse for analytics events.

## Table of contents
- `get_aws_bucket`
- `create_file_query`
- `create_file_chunks`
- `get_file_query`
- `get_dataset_file_query`
- `delete_file_query`

## References
- `super::chunk_operator::{create_chunk_metadata, get_row_count_for_organization_id_query}`
- `super::clickhouse_operator::{ClickHouseEvent, EventQueue}`
- `super::group_operator::{create_group_from_file_query, create_groups_query}`
- `super::parse_operator::{build_chunking_regex, coarse_doc_chunker, convert_html_to_text}`
- `crate::data::models::*`
- `crate::handlers::chunk_handler::ChunkReqPayload`
- `crate::handlers::file_handler::UploadFileReqPayload`
- `crate::errors::ServiceError`

## Symbols

### `get_aws_bucket`
#### Description
Retrieves an S3 bucket instance based on environment variables.

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;Bucket, ServiceError&gt; | An S3 bucket instance or a ServiceError if bucket creation fails. |

#### Internal Logic
- Reads environment variables for S3 endpoint, bucket name, and credentials.
- Creates an S3 region based on the endpoint and region name.
- Attempts to retrieve credentials from instance metadata, falling back to environment variables if necessary.
- Creates an S3 bucket instance using the provided information.
- Configures the bucket to use path-style URLs.

### `create_file_query`
#### Description
Inserts a new file record into the database.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| file_id | uuid::Uuid | The UUID of the file. |
| file_size | i64 | The size of the file in bytes. |
| upload_file_data | UploadFileReqPayload | The payload containing file details. |
| dataset_id | uuid::Uuid | The UUID of the dataset the file belongs to. |
| pool | web::Data&lt;Pool&gt; | A database connection pool. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;File, ServiceError&gt; | The created file record or a ServiceError if insertion fails. |

#### Internal Logic
- Acquires a database connection from the pool.
- Creates a new `File` instance from the provided details.
- Inserts the new file record into the `files` table.

### `create_file_chunks`
#### Description
Processes an uploaded file, chunks its content, and queues the chunks for ingestion.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| created_file_id | uuid::Uuid | The UUID of the newly created file. |
| upload_file_data | UploadFileReqPayload | The payload containing file details. |
| html_content | String | The HTML content of the file. |
| dataset_org_plan_sub | DatasetAndOrgWithSubAndPlan | Dataset, organization, plan, and subscription details. |
| pool | web::Data&lt;Pool&gt; | A database connection pool. |
| event_queue | web::Data&lt;EventQueue&gt; | A queue for ClickHouse events. |
| redis_conn | MultiplexedConnection | A Redis connection. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;(), ServiceError&gt; | A Result indicating success or a ServiceError if processing fails. |

#### Internal Logic
- Converts the HTML content to plain text.
- Builds a chunking regex based on provided delimiters.
- Splits the file content into chunks using the `coarse_doc_chunker` function.
- Creates a new `ChunkGroup` for the file.
- Inserts the `ChunkGroup` into the database.
- Creates a `FileGroup` association between the file and the group.
- Iterates through the chunks:
    - Creates a `ChunkReqPayload` for each chunk.
    - Appends the payload to a list.
- Checks if the total chunk count exceeds the organization's plan limit.
- Retrieves the dataset configuration.
- Splits the chunk payloads into segments for batch processing.
- Serializes the chunk segments into JSON messages.
- Pushes the serialized messages to the Redis queue for ingestion.
- Sends a `FileUploaded` event to the ClickHouse event queue.

### `get_file_query`
#### Description
Retrieves a file record from the database and generates a pre-signed S3 URL for downloading it.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| file_uuid | uuid::Uuid | The UUID of the file. |
| dataset_id | uuid::Uuid | The UUID of the dataset the file belongs to. |
| pool | web::Data&lt;Pool&gt; | A database connection pool. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;FileDTO, actix_web::Error&gt; | A FileDTO containing file details and the pre-signed S3 URL, or an actix_web::Error if retrieval fails. |

#### Internal Logic
- Acquires a database connection from the pool.
- Retrieves the file record from the `files` table based on the file UUID and dataset ID.
- Generates a pre-signed S3 URL for the file with a custom "Content-Disposition" header for downloading.
- Creates a `FileDTO` instance from the file record and the pre-signed URL.

### `get_dataset_file_query`
#### Description
Retrieves a paginated list of files belonging to a specific dataset.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| dataset_id | uuid::Uuid | The UUID of the dataset. |
| page | u64 | The page number for pagination. |
| pool | web::Data&lt;Pool&gt; | A database connection pool. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;Vec&lt;(File, i64, Option&lt;uuid::Uuid&gt;)&gt;, actix_web::Error&gt; | A vector of tuples containing file records, total count, and associated group ID, or an actix_web::Error if retrieval fails. |

#### Internal Logic
- Acquires a database connection from the pool.
- Queries the `files` table, joining with `groups_from_files` to retrieve associated group IDs.
- Filters the query by dataset ID.
- Applies pagination limits and offsets.
- Returns the retrieved file records, total count, and group IDs.

### `delete_file_query`
#### Description
Deletes a file record from the database and removes the corresponding file from S3.

#### Inputs
| Name | Type | Description |
|:-----|:-----|:------------|
| file_uuid | uuid::Uuid | The UUID of the file. |
| dataset | Dataset | The dataset the file belongs to. |
| pool | web::Data&lt;Pool&gt; | A database connection pool. |
| dataset_config | DatasetConfiguration | The dataset configuration. |

#### Outputs
| Name | Type | Description |
|:-----|:-----|:------------|
| result | Result&lt;(), actix_web::Error&gt; | A Result indicating success or an actix_web::Error if deletion fails. |

#### Internal Logic
- Acquires a database connection from the pool.
- Retrieves the file record from the `files` table.
- Deletes the file from the S3 bucket.
- Performs a database transaction:
    - Deletes the file record from the `files` table.
- Returns a success result.

## Dependencies
- `diesel`
- `diesel_async`
- `redis`
- `s3`
- `regex`
- `scraper`
- `sentry`
- `tracing`
- `actix_web`

## Error Handling
The functions in this file return `ServiceError` for various error conditions, such as database errors, S3 errors, and invalid input.

## Logging
The functions use the `log` crate for logging errors and informational messages.

## Future Improvements
- Implement more robust error handling, potentially with specific error types for different failure scenarios.
- Consider adding support for other file formats beyond HTML.
- Explore options for optimizing file chunking and ingestion performance.
- Add more detailed logging for debugging and monitoring purposes.

```

```
